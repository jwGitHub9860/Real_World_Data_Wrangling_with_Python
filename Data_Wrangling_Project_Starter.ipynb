{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNIwe5N7s0e_"
   },
   "source": [
    "# Real-world Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BG63Tpg8ep_"
   },
   "source": [
    "In this project, you will apply the skills you acquired in the course to gather and wrangle real-world data with two datasets of your choice.\n",
    "\n",
    "You will retrieve and extract the data, assess the data programmatically and visually, accross elements of data quality and structure, and implement a cleaning strategy for the data. You will then store the updated data into your selected database/data store, combine the data, and answer a research question with the datasets.\n",
    "\n",
    "Throughout the process, you are expected to:\n",
    "\n",
    "1. Explain your decisions towards methods used for gathering, assessing, cleaning, storing, and answering the research question\n",
    "2. Write code comments so your code is more readable\n",
    "\n",
    "Before you start, install the some of the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle==1.6.12\n",
      "  Downloading kaggle-1.6.12.tar.gz (79 kB)\n",
      "     ---------------------------------------- 0.0/79.7 kB ? eta -:--:--\n",
      "     ----- ---------------------------------- 10.2/79.7 kB ? eta -:--:--\n",
      "     -------------- ----------------------- 30.7/79.7 kB 435.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 79.7/79.7 kB 738.5 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\jwori\\anaconda3\\lib\\site-packages (from kaggle==1.6.12) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in c:\\users\\jwori\\anaconda3\\lib\\site-packages (from kaggle==1.6.12) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\jwori\\anaconda3\\lib\\site-packages (from kaggle==1.6.12) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in c:\\users\\jwori\\anaconda3\\lib\\site-packages (from kaggle==1.6.12) (2.32.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jwori\\anaconda3\\lib\\site-packages (from kaggle==1.6.12) (4.66.4)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\jwori\\anaconda3\\lib\\site-packages (from kaggle==1.6.12) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\jwori\\anaconda3\\lib\\site-packages (from kaggle==1.6.12) (2.2.2)\n",
      "Requirement already satisfied: bleach in c:\\users\\jwori\\anaconda3\\lib\\site-packages (from kaggle==1.6.12) (4.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jwori\\anaconda3\\lib\\site-packages (from bleach->kaggle==1.6.12) (23.2)\n",
      "Requirement already satisfied: webencodings in c:\\users\\jwori\\anaconda3\\lib\\site-packages (from bleach->kaggle==1.6.12) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\jwori\\anaconda3\\lib\\site-packages (from python-slugify->kaggle==1.6.12) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jwori\\anaconda3\\lib\\site-packages (from requests->kaggle==1.6.12) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jwori\\anaconda3\\lib\\site-packages (from requests->kaggle==1.6.12) (3.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\jwori\\anaconda3\\lib\\site-packages (from tqdm->kaggle==1.6.12) (0.4.6)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.6.12-py3-none-any.whl size=102982 sha256=d82cc1e2266a4fc7440868e32d9297cec842a8e4d1c787aeb2e85eb23d0dbd10\n",
      "  Stored in directory: c:\\users\\jwori\\appdata\\local\\pip\\cache\\wheels\\2a\\00\\63\\fa8dbcfb1458e6a65ac7a28d456deb9e0e033245b67f952681\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "Successfully installed kaggle-1.6.12\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install kaggle==1.6.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --target=/workspace ucimlrepo numpy==1.24.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Restart the kernel to use updated package(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDYDkH-Zs7Nn"
   },
   "source": [
    "## 1. Gather data\n",
    "\n",
    "In this section, you will extract data using two different data gathering methods and combine the data. Use at least two different types of data-gathering methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbN7z7rcuqpO"
   },
   "source": [
    "### **1.1.** Problem Statement\n",
    "In 2-4 sentences, explain the kind of problem you want to look at and the datasets you will be wrangling for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gi6swhjSYqu2"
   },
   "source": [
    "Finding the right datasets can be time-consuming. Here we provide you with a list of websites to start with. But we encourage you to explore more websites and find the data that interests you.\n",
    "\n",
    "* Google Dataset Search https://datasetsearch.research.google.com/\n",
    "* The U.S. Government’s open data https://data.gov/\n",
    "* UCI Machine Learning Repository https://archive.ics.uci.edu/ml/index.php\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many factors that cause pollution. Some factors are well-known while other factors are not. One factor that is not widely known is population. The project will be addressing the following research question: **Do the most polluted cities have the largest populations?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AQfBAdUypMm"
   },
   "source": [
    "### **1.2.** Gather at least two datasets using two different data gathering methods\n",
    "\n",
    "List of data gathering methods:\n",
    "\n",
    "- Download data manually\n",
    "- Programmatically downloading files\n",
    "- Gather data by accessing APIs\n",
    "- Gather and extract data from HTML files using BeautifulSoup\n",
    "- Extract data from a SQL database\n",
    "\n",
    "Each dataset must have at least two variables, and have greater than 500 data samples within each dataset.\n",
    "\n",
    "For each dataset, briefly describe why you picked the dataset and the gathering method (2-3 full sentences), including the names and significance of the variables in the dataset. Show your work (e.g., if using an API to download the data, please include a snippet of your code). \n",
    "\n",
    "Load the dataset programmtically into this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset 1**\n",
    "\n",
    "**_Type_**: CSV File\n",
    "\n",
    "**_Method_**: The data was gathered using the \"_Downloading data manually_\" method from the **\"edit_AIR QUALITY INDEX (by cities) - IQAir\"** CSV file.\n",
    "\n",
    "The method of \"_Downloading data manually_\" was used to gather the data from the first dataset because it is the simplest and easiest method to use to gather data from a dataset while keeping an original and edited version of the dataset out of all five methods. The method requires the file of the dataset to be downloaded into the location where all the project files are being held first. Finally, only one line of code is used to read and access the data from the file of the dataset. To keep an original and edited version of the dataset, the dataset simply needs to be downloaded twice and kept under different names in the location where all the project files are being held. This shows that the method of \"_Downloading data manually_\" is the simplest and easiest method out of all five methods.  \n",
    "\n",
    "**_Dataset variables_**:\n",
    "\n",
    "*   *_**\"2019\"**_: Air Quality Index (AQI) values for measuring Pollution during 2019*\n",
    "*   *_**\"City\"**_: Both Country and City names*\n",
    "\n",
    "The **\"2019\"** column is needed because it contains the air quality index values that measure the pollution of different cities during 2019. The air quality index values during 2019 are the only air quality index values that can be shared with the other dataset because the other dataset only has values from 2019. These values are also needed to answer the research question since they have values of pollution.\n",
    "\n",
    "The **\"City\"** column is needed because the data in the column will be used to merge the two datasets together after the **city** names are put into a new column that is separate from the column containing the **country** names.\n",
    "\n",
    "The rest of the columns containing _rank_ and the air quality index values for each year from _2017_ to _2021_, except _2019_, are not needed to answer the research question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Su8E0uLuYkHU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>City</th>\n",
       "      <th>2021</th>\n",
       "      <th>JAN(2021)</th>\n",
       "      <th>FEB(2021)</th>\n",
       "      <th>MAR(2021)</th>\n",
       "      <th>APR(2021)</th>\n",
       "      <th>MAY(2021)</th>\n",
       "      <th>JUN(2021)</th>\n",
       "      <th>JUL(2021)</th>\n",
       "      <th>AUG(2021)</th>\n",
       "      <th>SEP(2021)</th>\n",
       "      <th>OCT(2021)</th>\n",
       "      <th>NOV(2021)</th>\n",
       "      <th>DEC(2021)</th>\n",
       "      <th>2020</th>\n",
       "      <th>2019</th>\n",
       "      <th>2018</th>\n",
       "      <th>2017</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bhiwadi, India</td>\n",
       "      <td>106.2</td>\n",
       "      <td>145.8</td>\n",
       "      <td>129.8</td>\n",
       "      <td>120.2</td>\n",
       "      <td>125.7</td>\n",
       "      <td>86.5</td>\n",
       "      <td>95.9</td>\n",
       "      <td>55.6</td>\n",
       "      <td>55.4</td>\n",
       "      <td>37.1</td>\n",
       "      <td>91.1</td>\n",
       "      <td>188.6</td>\n",
       "      <td>136.6</td>\n",
       "      <td>95.5</td>\n",
       "      <td>83.4</td>\n",
       "      <td>125.4</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ghaziabad, India</td>\n",
       "      <td>102.0</td>\n",
       "      <td>199.9</td>\n",
       "      <td>172.2</td>\n",
       "      <td>97.8</td>\n",
       "      <td>86.3</td>\n",
       "      <td>52.9</td>\n",
       "      <td>47.2</td>\n",
       "      <td>35.3</td>\n",
       "      <td>37.6</td>\n",
       "      <td>30.8</td>\n",
       "      <td>89.7</td>\n",
       "      <td>218.3</td>\n",
       "      <td>163</td>\n",
       "      <td>106.6</td>\n",
       "      <td>110.2</td>\n",
       "      <td>135.2</td>\n",
       "      <td>144.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Hotan, China</td>\n",
       "      <td>101.5</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>158</td>\n",
       "      <td>91.1</td>\n",
       "      <td>167.4</td>\n",
       "      <td>57.4</td>\n",
       "      <td>70.9</td>\n",
       "      <td>93.2</td>\n",
       "      <td>79.3</td>\n",
       "      <td>126.1</td>\n",
       "      <td>111.5</td>\n",
       "      <td>62.6</td>\n",
       "      <td>110.2</td>\n",
       "      <td>110.1</td>\n",
       "      <td>116</td>\n",
       "      <td>91.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Delhi, India</td>\n",
       "      <td>96.4</td>\n",
       "      <td>183.7</td>\n",
       "      <td>142.2</td>\n",
       "      <td>80.5</td>\n",
       "      <td>72.9</td>\n",
       "      <td>47.4</td>\n",
       "      <td>47.1</td>\n",
       "      <td>35.6</td>\n",
       "      <td>36.9</td>\n",
       "      <td>30.2</td>\n",
       "      <td>73.7</td>\n",
       "      <td>224.1</td>\n",
       "      <td>186.4</td>\n",
       "      <td>84.1</td>\n",
       "      <td>98.6</td>\n",
       "      <td>113.5</td>\n",
       "      <td>108.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Jaunpur, India</td>\n",
       "      <td>95.3</td>\n",
       "      <td>182.2</td>\n",
       "      <td>143.5</td>\n",
       "      <td>91</td>\n",
       "      <td>70</td>\n",
       "      <td>51.1</td>\n",
       "      <td>40.7</td>\n",
       "      <td>33.5</td>\n",
       "      <td>34.2</td>\n",
       "      <td>36.8</td>\n",
       "      <td>75.7</td>\n",
       "      <td>196</td>\n",
       "      <td>195.7</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6470</th>\n",
       "      <td>6471</td>\n",
       "      <td>Mornington, Australia</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6471</th>\n",
       "      <td>6472</td>\n",
       "      <td>Emu River, Australia</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6472</th>\n",
       "      <td>6473</td>\n",
       "      <td>Judbury, Australia</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6473</th>\n",
       "      <td>6474</td>\n",
       "      <td>St Helens, Australia</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6474</th>\n",
       "      <td>6475</td>\n",
       "      <td>Chu, Kazakhstan</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6475 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rank                   City   2021 JAN(2021) FEB(2021) MAR(2021)  \\\n",
       "0        1         Bhiwadi, India  106.2     145.8     129.8     120.2   \n",
       "1        2       Ghaziabad, India  102.0     199.9     172.2      97.8   \n",
       "2        3           Hotan, China  101.5         -         -       158   \n",
       "3        4           Delhi, India   96.4     183.7     142.2      80.5   \n",
       "4        5         Jaunpur, India   95.3     182.2     143.5        91   \n",
       "...    ...                    ...    ...       ...       ...       ...   \n",
       "6470  6471  Mornington, Australia    2.4         2       1.9       2.3   \n",
       "6471  6472   Emu River, Australia    2.1       1.9       1.8         2   \n",
       "6472  6473     Judbury, Australia    2.0       1.6       1.5       2.1   \n",
       "6473  6474   St Helens, Australia    1.9       1.8       2.1         2   \n",
       "6474  6475        Chu, Kazakhstan    1.5       1.5       1.6       1.6   \n",
       "\n",
       "     APR(2021) MAY(2021) JUN(2021) JUL(2021) AUG(2021) SEP(2021) OCT(2021)  \\\n",
       "0        125.7      86.5      95.9      55.6      55.4      37.1      91.1   \n",
       "1         86.3      52.9      47.2      35.3      37.6      30.8      89.7   \n",
       "2         91.1     167.4      57.4      70.9      93.2      79.3     126.1   \n",
       "3         72.9      47.4      47.1      35.6      36.9      30.2      73.7   \n",
       "4           70      51.1      40.7      33.5      34.2      36.8      75.7   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6470       2.1       3.2       3.6       4.3       2.1       2.1       1.7   \n",
       "6471       2.6       3.4       2.6       1.9       2.1       2.2       1.5   \n",
       "6472       1.5       4.1         2       2.2       2.2       1.7       1.5   \n",
       "6473       2.4       2.7       1.6       1.6       1.6       1.9       1.6   \n",
       "6474       1.5       1.5       1.5       1.5       1.6       1.5       1.5   \n",
       "\n",
       "     NOV(2021) DEC(2021)   2020   2019   2018   2017  \n",
       "0        188.6     136.6   95.5   83.4  125.4      -  \n",
       "1        218.3       163  106.6  110.2  135.2  144.6  \n",
       "2        111.5      62.6  110.2  110.1    116   91.9  \n",
       "3        224.1     186.4   84.1   98.6  113.5  108.2  \n",
       "4          196     195.7      -      -      -      -  \n",
       "...        ...       ...    ...    ...    ...    ...  \n",
       "6470         2       1.9    3.2    3.8      3    3.9  \n",
       "6471       1.4       1.5    2.6    2.5    2.6    2.3  \n",
       "6472       1.4       1.7    2.4    5.7    2.2    1.9  \n",
       "6473       2.4       1.6    2.4    2.4    2.9    3.3  \n",
       "6474       1.5       1.5      -      -      -      -  \n",
       "\n",
       "[6475 rows x 19 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FILL IN 1st data gathering and loading method\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_polluted_cities_and_countries = pd.read_csv(\"edit_AIR QUALITY INDEX (by cities) - IQAir.csv\") # loads CSV file of first dataset\n",
    "df_polluted_cities_and_countries # displays first dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 2\n",
    "\n",
    "**_Type_**: CSV File\n",
    "\n",
    "**_Method_**: The data was gathered using the \"Programmatically download files\" method from the **\"worldcities\"** CSV file.\n",
    "\n",
    "The method of \"_Programmatically download files_\" was used to gather the data from the second dataset because it is the next simplest and easiest method to use to gather data from a dataset \n",
    "out of all five methods. The method first requires the zip file of the dataset to be downloaded into the location where all the project files are being held. Next, two lines of code are used to unzip the zip file in read mode. Finally, the data from the file of the second dataset in the zip file is accessed using the same gathering method used for the first dataset. This shows that the method of \"_Programmatically download files_\" is the next simplest and easiest method out of all five methods.\n",
    "\n",
    "**_Dataset variables_**:\n",
    "\n",
    "*   *_**\"population\"**_: Populations of cities within countries during 2019*\n",
    "*   *_**\"city\"**_: City names*\n",
    "*   *_**\"country\"**_: Country names*\n",
    "\n",
    "The **\"population\"** column is needed because it contains the city populations during 2019. This population data can be shared with the other dataset because the other dataset has values covering from 2017 to 2021. The populations during 2019 are within the time range. The population data is also needed to answer the research question since the research question requires information about the city populations.\n",
    "\n",
    "The **\"city\"** and **\"country\"** columns are needed because the data in the columns will be used to merge the two datasets together.\n",
    "\n",
    "The rest of the columns containing the _city ASCII string_, _city latitude_, _city longitude_, _alpha-2 country code_, _alpha-3 country code_, _city admin name_, and country's _capital_ are not needed to answer the research question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6zT0QxRyYmm7"
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 3 column 1 (char 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:974\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 3 column 1 (char 4)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m api_world_cities_dataset \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url) \u001b[38;5;66;03m# extracts data from \"url\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m api_world_cities_dataset\u001b[38;5;241m.\u001b[39mraise_for_status() \u001b[38;5;66;03m# raises exception if request results in error\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m api_world_cities_dataset_text \u001b[38;5;241m=\u001b[39m api_world_cities_dataset\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:978\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 978\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 3 column 1 (char 4)"
     ]
    }
   ],
   "source": [
    "#FILL IN 2nd data gathering and loading method\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'https://www.kaggle.com/datasets/viswanathanc/world-cities-datasets' # url of second dataset\n",
    "api_world_cities_dataset = requests.get(url) # extracts data from \"url\"\n",
    "api_world_cities_dataset.raise_for_status() # raises exception if request results in error\n",
    "api_world_cities_dataset_text = api_world_cities_dataset.json() # obtains JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional data storing step: You may save your raw dataset files to the local data store before moving to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional: store the raw data in your local data store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwSWIVmotLgV"
   },
   "source": [
    "## 2. Assess data\n",
    "\n",
    "Assess the data according to data quality and tidiness metrics using the report below.\n",
    "\n",
    "List **two** data quality issues and **two** tidiness issues. Assess each data issue visually **and** programmatically, then briefly describe the issue you find.  **Make sure you include justifications for the methods you use for the assessment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adaK2iPNzVu4"
   },
   "source": [
    "### Quality Issue 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpW59kh-zl8d"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qfcocStzsKg"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue and justification: *FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Be77N4I1AmE"
   },
   "source": [
    "### Quality Issue 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMhHyiyLM2I3"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnviRCUI-bb7"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue and justification: *FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXhGiYyiwwKN"
   },
   "source": [
    "### Tidiness Issue 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fleC5rORI0Xl"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTuQw7Rbsio4"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue and justification: *FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ffMoRGSwzYj"
   },
   "source": [
    "### Tidiness Issue 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUpeoqokw5Qt"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8JK4DoXxtFA"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue and justification: *FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6gmLnBttpCh"
   },
   "source": [
    "## 3. Clean data\n",
    "Clean the data to solve the 4 issues corresponding to data quality and tidiness found in the assessing step. **Make sure you include justifications for your cleaning decisions.**\n",
    "\n",
    "After the cleaning for each issue, please use **either** the visually or programatical method to validate the cleaning was succesful.\n",
    "\n",
    "At this stage, you are also expected to remove variables that are unnecessary for your analysis and combine your datasets. Depending on your datasets, you may choose to perform variable combination and elimination before or after the cleaning stage. Your dataset must have **at least** 4 variables after combining the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN - Make copies of the datasets to ensure the raw dataframes \n",
    "# are not impacted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmFhN52Yyn3l"
   },
   "source": [
    "### **Quality Issue 1: FILL IN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UejDWrNMW4a"
   },
   "outputs": [],
   "source": [
    "# FILL IN - Apply the cleaning strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUBee-LPytkv"
   },
   "outputs": [],
   "source": [
    "# FILL IN - Validate the cleaning was successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification: *FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_DAUbJrymBL"
   },
   "source": [
    "### **Quality Issue 2: FILL IN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Yfb-Yu5MTuE"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Apply the cleaning strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ionB2sRaMUmY"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Validate the cleaning was successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification: *FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIUrrfSNyOPR"
   },
   "source": [
    "### **Tidiness Issue 1: FILL IN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fib0zAm333bn"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Apply the cleaning strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhrnUGY_Nk8B"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Validate the cleaning was successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification: *FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o51Bt8kwyTzk"
   },
   "source": [
    "### **Tidiness Issue 2: FILL IN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zW8O5yx4Y9O"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Apply the cleaning strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6I_Sr7lxXi5"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Validate the cleaning was successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification: *FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Remove unnecessary variables and combine datasets**\n",
    "\n",
    "Depending on the datasets, you can also peform the combination before the cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILL IN - Remove unnecessary variables and combine datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F42urHuzttjF"
   },
   "source": [
    "## 4. Update your data store\n",
    "Update your local database/data store with the cleaned data, following best practices for storing your cleaned data:\n",
    "\n",
    "- Must maintain different instances / versions of data (raw and cleaned data)\n",
    "- Must name the dataset files informatively\n",
    "- Ensure both the raw and cleaned data is saved to your database/data store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3uay7EJUV_L"
   },
   "outputs": [],
   "source": [
    "#FILL IN - saving data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGy_yddGtzhM"
   },
   "source": [
    "## 5. Answer the research question\n",
    "\n",
    "### **5.1:** Define and answer the research question \n",
    "Going back to the problem statement in step 1, use the cleaned data to answer the question you raised. Produce **at least** two visualizations using the cleaned data and explain how they help you answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjedE4s4ZkEd"
   },
   "source": [
    "*Research question:* FILL IN from answer to Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lkw3rW9kZmOm"
   },
   "outputs": [],
   "source": [
    "#Visual 1 - FILL IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer to research question:* FILL IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fdK_8ZGZm9R"
   },
   "outputs": [],
   "source": [
    "#Visual 2 - FILL IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5RgvMGUZoHn"
   },
   "source": [
    "*Answer to research question:* FILL IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ezWXXZVj-TP"
   },
   "source": [
    "### **5.2:** Reflection\n",
    "In 2-4 sentences, if you had more time to complete the project, what actions would you take? For example, which data quality and structural issues would you look into further, and what research questions would you further explore?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XB3RBDG5kFe1"
   },
   "source": [
    "*Answer:* FILL IN"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
